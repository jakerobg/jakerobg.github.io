[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Jake Gilbert is a recent Computer Science and Film gradute from Middlebury College, interested in the intersection of technology, visual storytelling, design and activism. Follow his Github\nThis site was built using Quarto. All blogposts were made with Python and Jupyter Notebook."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nimport torch\nimport matplotlib.pyplot as plt\nfrom perceptron import Perceptron, PerceptronOptimizer, PerceptronOptimizer_minibatch, Perceptron_minibatch\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/perceptron/index.html#experiment-1",
    "href": "posts/perceptron/index.html#experiment-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 1",
    "text": "Experiment 1\nWe already have the code that plots out the perceptron data and generates points, now we need code to draw the line that can separate it.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nFor this experiment, we want to run the training loop but keep track of the losses in order to visualize how the algorithm changes teh location of the hyperplane to eventually separate the data. The dashed line is from the step before, and the data point that is circled shows which point is used to update. Ov\n\ntorch.manual_seed(1234567) # This code is from the lecture by Professor Chodrow\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # pick rand point\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWhen we keep track of the loss, we can see that we can visualize how the loss eventually reaches zero and the hyperplane separates the data.\nWe can also plot the loss overtime using our loss_vec to see how it eventually reaches zero over a certain amount of iterations:\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")"
  },
  {
    "objectID": "posts/perceptron/index.html#experiment-2",
    "href": "posts/perceptron/index.html#experiment-2",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 2",
    "text": "Experiment 2\nIf instead we generate new data that is not linearly separable, we can visualize how the loss never reaches zero and the weight value never reaches an optimal state to separate the data:\n\ntorch.manual_seed(1234)\n\nX, y = perceptron_data(n_points = 300, noise = 0.35)\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nWe can run a similar training loop, but because we know the loss will never converge at 0, we can set the limit iterations to 500.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\niteration = 0\n\nwhile  loss &gt; 0 and iteration &lt; 500:\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    iteration += 1\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nThe line seems to try to separate the data after 500 iterations, but the loss is not 0, although it can be close:\n\nloss.item()\n\n0.04333333298563957\n\n\nWe can visualize the loss:\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")"
  },
  {
    "objectID": "posts/perceptron/index.html#experiment-3",
    "href": "posts/perceptron/index.html#experiment-3",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 3",
    "text": "Experiment 3\nWe can also experiment with the perceptron algorithm in more than 2 dimensions. We can run the following code on data with 5 features.\n\ntorch.manual_seed(1234)\n\ndef perceptron_data_dimensions(n_points = 300, noise = 0.35, dimensions = 5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, dimensions))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data_dimensions(n_points = 300, noise = 0.35, dimensions = 5)\n\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\niteration = 0\n\nwhile iteration &lt; 1000 and loss &gt; 0:\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    iteration += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThe code above shows the loss converges to 0 after only 5 iterations, showing this multi-dimensional data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/index.html#experiment-2-1",
    "href": "posts/perceptron/index.html#experiment-2-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 2",
    "text": "Experiment 2\nNow with a new K value of 10 and with 5 dimensions, we need to see if the data is linearly separable:\n\nX, y = perceptron_data_dimensions(n_points = 300, noise = 0.2, dimensions=5)\n\n# instantiate a model and an optimizer\np = Perceptron_minibatch() \nopt = PerceptronOptimizer_minibatch(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n\n\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Loss Graph: Experiment 2 with minibatch')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"loss: \", loss_vec[-1])\n\n\n\n\n\n\n\n\nloss:  tensor(0.)\n\n\nIt seems that our final loss is 0 after qualified leader, showing the data is linearly separable with minibatch at high dimensions adn k = 10"
  },
  {
    "objectID": "posts/perceptron/index.html#experiment-3-1",
    "href": "posts/perceptron/index.html#experiment-3-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 3",
    "text": "Experiment 3\nFinally, we will see if the data is linearly separable if k = n — or the entire size of the feature list\n\nX, y = perceptron_data(n_points = 300, noise = 0.5)\n# instantiate a model and an optimizer\np = Perceptron_minibatch() \nopt = PerceptronOptimizer_minibatch(p)\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\niterations = 0\nwhile iterations &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = n\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n        \n    iterations += 1\n\n\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Experiemnt 3 with minibatch perceptron')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"loss: \", loss_vec[-1])\n\n\n\n\n\n\n\n\nloss:  0.07333333045244217\n\n\nThe data is not linearly separable, as the loss function did not converge on zero."
  },
  {
    "objectID": "posts/music-classification/index.html",
    "href": "posts/music-classification/index.html",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "import torch\nimport pandas as pd\n\nimport numpy as np\n\n# for embedding visualization later\nimport plotly.express as px \nimport plotly.io as pio\n\n# for VSCode plotly rendering\npio.renderers.default = \"notebook\"\n\n# for appearance\npio.templates.default = \"plotly_white\"\n\n# for train-test split\nfrom sklearn.model_selection import train_test_split\n\n# for suppressing bugged warnings from torchinfo\nimport warnings \nwarnings.filterwarnings(\"ignore\", category = UserWarning)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']\n\ndf.groupby(\"genre\").size()\n\ngenre\nblues      4604\ncountry    5445\nhip hop     904\njazz       3845\npop        7042\nreggae     2498\nrock       4034\ndtype: int64\n\n\n\ngenres = {\n    \"blues\": 0,\n    \"country\": 1,\n    \"hip hop\": 2,\n    \"jazz\": 3,\n    \"pop\": 4,\n    \"reggae\": 5,\n    \"rock\": 6\n}\n\ndf[\"genre\"] = df[\"genre\"].apply(genres.get)\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\n4\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\n4\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\n4\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\n4\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\n4\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n    \n    def __getitem__(self, index):\n        return self.df.iloc[index, 5], self.df.iloc[index, 0]\n\n    def __len__(self):\n        return len(self.df)                \n\n\ndf_train, df_val = train_test_split(df,shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data   = TextDataFromDF(df_val)\n\n\ntrain_data[194]\n\n('morning ride think morning ride morning ride nice ride miss morning ride longest ride morning ride morning ride morning ride morning ride slip slide go break slip slide go break matter hide send send morning ride morning ride morning ride morning ride tell ellington work jamaica buerue credit station gemini port portland morning ride morning ride morning ride morning ride',\n 64271)\n\n\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\ntokenized = tokenizer(train_data[194][0])\ntokenized\n\nOSError: dlopen(/Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN2at4_ops15to_dtype_layout4callERKNS_6TensorENSt3__18optionalIN3c1010ScalarTypeEEENS6_INS7_6LayoutEEENS6_INS7_6DeviceEEENS6_IbEEbbNS6_INS7_12MemoryFormatEEE\n  Referenced from: &lt;B145C7C7-A04C-3975-B142-8B160ADC1CFF&gt; /Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     &lt;6B754090-A299-3FA1-B21D-A3C9B7051AD1&gt; /Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib"
  },
  {
    "objectID": "posts/project-blog-post/index.html",
    "href": "posts/project-blog-post/index.html",
    "title": "UNCHR Machine Learning Challenge",
    "section": "",
    "text": "Jamie Hackney, Mihir Singh, Jake Gilbert\n\n\nThis project seeks to develop, implement, and evaluate various machine learning models to accurately predict refugee population movements in Somalia as part of the United Nations Commissioner for Refugees (UNHCR) Machine Learning Challenge. The data used by our models’ was collected by the UNHCR between 2015 and 2024 in Somalia. It contains information on population movements, markets, health, conflicts, and climate. We developed a web scraper to collect this data and combine it all into a single dataframe. To deal with missing data values, we created two datasets, one by dropping the missing values and the other by imputing them. Then, we built a Logistic Regression, Decision Tree, and Random Forest model on different subsets of these two datasets. We tested these models to see how accurately they could predict the number of refugee arrivals into a specific district in Somalia by computing the root mean squared error (RMSE), framing it as a regression problem. We found that the models had, on average, very high root mean squared errors (indicating a poor fit on the data), yet also had high R2 values (indicating a good fit on the data). We believe this discrepancy is explained by the fact that there are several very large outliers present in the data. To validate this, we binned the predictions into the three severity levels proposed by the UN, and treated it as a classification problem. When doing so, our best model, a Decision Tree Regressor trained on a subset of the dataset where missing features were dropped, achieved an accuracy of 99.5%. This model was also able to perfectly predict the exact number of arrivals 98% of the time, further supporting our hypothesis that these models fit the data very well and the high RMSE is explained by the existence of several large outliers.\n\n\n\nThe intended users of our project are the Somalian government and other NGOs that support refugees in the area. However, our project deals directly with people, as we are predicting where we expect refugees to move in Somalia. Because of this, the entire population of refugees in Somalia could potentially be affected by our project, if it were to be used by any of these agencies or governments to inform resource allocations. Both refugees in Somalia and the groups that support these refugees stand to benefit from our project, assuming the models are an effective tool. If this is the case, then these agencies can better allocate resources, which will benefit the refugees themselves as they will have access to more resources. Additionally, the government and agencies supporting them will benefit from more efficient resource movement. Conversely, if our models prove to be inaccurate enough that they cause resources to be misallocated, both refugees and the groups supporting them could be seriously harmed. Additionally, we note the possibility that bad actors could co-opt these models to predict refugee movements for malicious purposes.\nWe chose to work on this project because we were interested in the intersection between machine learning and public policy. When we found the UN challenge related to this project, we became very interested in the cause of this project, and found the ability to help refugees and their support groups compelling. Assuming that our models are an effective tool for helping agencies allocate resources for refugees, we believe that the benefits to refugee populations and their support groups will bring some relief to that region.\n\n\n\nOur program addresses the humanitarian crisis in Somalia related to refugee movements and forcibly displaced people. The ability to accurately predict patterns in forcibly displaced peoples movements is crucial to the development of public policies that provide aid and resources to refugees. This is not a novel problem, and in fact has been approached several times before. Suleimenova et al., Gulden et al., and Lin et al. all developed agent based models to predict population movements. Gleick investigated the relationship between water and conflict in Syria, drawing a connection between water scarcity and conflicts, which in turn can drive population movements. Finally, Finnley noted an increase in migration of women and children during the severe drought of 1983-1985.\nWe focused on building models trained on a variety of data, including climate, conflicts, and market prices. By looking at a variety of conditions affecting human movement, we predict the movement patterns of displaced people in Somalia. The goal of this project is to effectively predict these population movements so that agencies supporting refugee populations in this area can better allocate resources to regions that are expected to receive a high number of migrants.\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom models import dropNA\nfrom models import impute\nfrom models import evaluate_LR\nfrom models import evaluate_DT\nfrom models import evaluate_RF\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nAll of our data came from the Food Security and Nutrition Analysis Unit Dashboard for Somalia (FSNAU). This organization collects data related to food and security in Somalia. FSNAU provides access to this data through a public facing dashboard. Users can select what data they would like to look at and which time frames (in six month increments).\nFor our data, focused on creating two datasets, one where we drop missing values and one where we impute them. Within these datasets, we further split them into all available features, and just the top nine features as recommended by the UNHCR in their previous work with this data (project Jetson). Due to data availability, we focused on data from 2015 - 2024.\n\n# load data\ndf = pd.read_csv('data/combined_data.csv')\n\n\n\n\nThe goal of our models was to predict the number of refugees arriving in a certain district in Somalia. We used the “Arrivals” column of our data as the target feature for our models to predict. For training features, we either trained our models on all features available on the FSNAU dashboard, or the nine features recommended by the UNHCR (region, district, month, year, rainfall, number of conflict fatalities, number of conflict incidents, water price, and goat price). We chose to run models trained on these nine features because the UN found these particularly helpful when building their own machine learning models (Jetson Data). For all models, we dropped the feature describing the number of departures because such data can only be collected after the fact. That is, at the time this model would be run in a “real” scenario, there would be no way of knowing the number of departures in a district for the entire month; a month must first pass for this data to become known. Finally, we split the data into train and test groups, with 80% of the data in the train split and 20% of the data in the test split. Depending on the dataset used, the exact number of rows varied, but for the dataset we found to be best (dropping missing values and only using the nine best features), we had 51,812 train rows and 12,953 test rows.\n\n# create four dataframes of the data\ndf_dropna = dropNA(df)\ndf_dropna_t9 = dropNA(df, top_9=True)\ndf_impute = impute(df)\ndf_impute_t9 = impute(df, top_9=True)\n\nWe trained a Linear Regression model, Decision Tree Regressor, and Random Forest Regressor model from Sci-Kit Learn to make our predictions. These models were chosen because predicting the number of refugees arriving is a regression problem, where we want our model to give us an estimate of the number of arrivals in a district. We trained each model on the four datasets we created: drop missing values and use all features, drop missing values and use only the best nine features, impute missing values and use all features, and impute missing values and use only the nine best features. The reason for choosing to both impute and drop missing values is to determine the best approach to deal with these missing values, of which there were many. In fact, out of the 328,856 available rows, only 230 had no missing values. A single missing value in any one feature can render the entire row obsolete in training or testing. The benefit of imputing the data is that we end up with much more data to train and test with, potentially increasing accuracy and decreasing the possibility of overfitting. The drawback is that the data is generally lower quality, as many values are estimates not actual observations. Dropping missing values, on the other hand, leads to a smaller dataset, potentially decreasing accuracy and increasing the possibility of overfitting. The data we are left with, however, is much higher quality since it is composed entirely of actual observations.\nThe four datasets that result from this process, as well as their sizes are below:\n\ndf_dropna includes all rows without a single Na or NaN in any feature.\n\ntrain size: 230 x 23\ntest size: 55 x 23\n\ndf_dropna_t9 includes all rows without a single Na or NaN in any of the top nine features.\n\ntrain size: 51,812 x 10\ntest size: 12,953 x 10\n\ndf_impute includes all rows and features and fills in missing values by estimating their value using SciKitLearn’s SimpleImputer.\n\ntrain size: 263084 x 23\ntest size: 65,772 x 23\n\ndf_impute_t9 includes all rows and only the best nine features, and fills in missing values by estimating their value using SciKitLearn’s SimpleImputer.\n\ntrain size: 263084 x 10\ntest size: 65,772 x 10\n\n\nThe disadvantage of imputing the data can be seen in the fact that it produces negative values in the “Arrivals” column, which is impossible.\nWe then evaluated each of the models on each data set using R2, RMSE, and the classification accuracy for the three alarm levels. This classification accuracy came from binning the predictions into one of the three population movement alarm levels set by FSNAU and calculating the percent of times the model predicted the correct alarm level. The reason we included this metric is because the dataset contained several large outliers that heavily skewed the RMSE, which we will go into more at the end of this section. Additionally, we figured that in a real world scenario, it is more likely the aid groups would care more about whether the incoming number of migrants is low, medium, or high, rather than an exact number.\n\n\n\n\nWe evaluate the models using three different metrics, R2, RMSE, and the alarm level classification accuracy. The R2 value measures how well the model explains the observed data values. RMSE measures the average error for a prediction, with the interpretation being that the RMSE value is the average number of people the predictions are off by. Finally, the classification accuracy measures how often the models correctly predict the alarm level. In other words, the classification accuracy is how often the models can correctly predict whether the arrivals will be less than 1000 people, between 1000 and 5000 people, or greater than 5000 people. The tables below show the model performances when evaluated with each of these metrics on the testing data.\n\n\n\n# dropna\nprint(\"Drop Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_dropna, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_LR(X_train, y_train, X_test, y_test)\n\n# dropna top 9\nprint(\"Drop Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_dropna_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_LR(X_train, y_train, X_test, y_test)\n\n# impute\nprint(\"Impute Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_impute, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_LR(X_train, y_train, X_test, y_test)\n\n# impute top 9\nprint(\"Impute Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_impute_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_LR(X_train, y_train, X_test, y_test)\n\nDrop Missing Values, Use All Features:\nscore: -0.5120080054803344\nrmse: 4004.408898337328\nclassification accuracy: 1.0\n\n\nDrop Missing Values, Use Best Nine Features:\nscore: 0.022127246581247872\nrmse: 5531.715825977417\nclassification accuracy: 0.46383077279394735\n\n\nImpute Missing Values, Use All Features:\nscore: 0.33164398083006263\nrmse: 3641.15860937481\nclassification accuracy: 0.6846834519248313\n\n\nImpute Missing Values, Use Best Nine Features:\nscore: 0.09979970878891353\nrmse: 4044.098286302458\nclassification accuracy: 0.6812321352551237\n\n\n\n\n\n\n\n\n# dropna\nprint(\"Drop Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_dropna, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_DT(X_train, y_train, X_test, y_test)\n\n# dropna top 9\nprint(\"Drop Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_dropna_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_DT(X_train, y_train, X_test, y_test)\n\n# impute\nprint(\"Impute Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_impute, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_DT(X_train, y_train, X_test, y_test)\n\n# impute top 9\nprint(\"Impute Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_impute_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_DT(X_train, y_train, X_test, y_test)\n\nDrop Missing Values, Use All Features:\nscore: -1.0293524315764553\nrmse: 1832.0020891214767\nclassification accuracy: 0.9454545454545454\n\n\nDrop Missing Values, Use Best Nine Features:\nscore: 0.9086362235559209\nrmse: 1686.7363933175686\nclassification accuracy: 0.9932834092488226\n\n\nImpute Missing Values, Use All Features:\nscore: 0.9141093025870605\nrmse: 1243.3944680840762\nclassification accuracy: 0.8610654989965335\n\n\nImpute Missing Values, Use Best Nine Features:\nscore: 0.8600721201782545\nrmse: 1622.8755875767067\nclassification accuracy: 0.8631940643434897\n\n\n\n\n\n\n\n\n# dropna\nprint(\"Drop Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_dropna, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_RF(X_train, y_train, X_test, y_test)\n\n# dropna top 9\nprint(\"Drop Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_dropna_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_RF(X_train, y_train, X_test, y_test)\n\n# impute\nprint(\"Impute Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_impute, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_RF(X_train, y_train, X_test, y_test)\n\n# impute top 9\nprint(\"Impute Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_impute_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_RF(X_train, y_train, X_test, y_test)\n\nDrop Missing Values, Use All Features:\nscore: -0.41771466376163113\nrmse: 1925.9235858153875\nclassification accuracy: 0.9272727272727272\n\n\nDrop Missing Values, Use Best Nine Features:\nscore: 0.963600818038985\nrmse: 1194.1962938556157\nclassification accuracy: 0.9922797807457732\n\n\nImpute Missing Values, Use All Features:\nscore: 0.9530521565094427\nrmse: 971.2710461948245\nclassification accuracy: 0.8653834458432159\n\n\nImpute Missing Values, Use Best Nine Features:\nscore: 0.9251482120105943\nrmse: 1137.0446510214172\nclassification accuracy: 0.8624186583956699\n\n\n\n\n\n\n\nThe dataset contained several large outliers for our target column (“Arrivals”) that were throwing off the RMSE calculations. The 95th percentile of arrivals was 3,701; in other words 90% of arrivales were 3,701 or fewer people. However, the maximum number of arrivals for a single observation was 259,678. This implies the existence of some large outliers in the dataset that skew the RMSE calculation, making the models seem to perform worse than they actually do.\n\nprint(f'90th percentile: \\n{df[\"Arrivals\"].quantile(q=0.95)} \\n')\nprint(f'arrivals statistics: \\n{df[\"Arrivals\"].describe()}')\n\n90th percentile: \n3701.0 \n\narrivals statistics: \ncount    191416.000000\nmean       1226.451807\nstd        5534.244627\nmin           1.000000\n25%          49.000000\n50%         194.000000\n75%         614.000000\nmax      259678.000000\nName: Arrivals, dtype: float64\n\n\n\n\n\nWhen we impute the data, we are filling in the missing values with an estimate based on the other present features. However, when we impute this dataset, we actually end up with some negative values in the arrival column, which is impossible. This can be problematic because it can teach the models to predict negative arrivals, which are impossible observations. We can see that 16% of our imputed data is negative.\n\ndef count_negative(values):\n    count = 0\n    for value in values:\n        if value &lt; 0:\n            count += 1\n    return count\n\nprint(f\"percent of imputed arrival data that is negative: {count_negative(df_impute['Arrivals'])/ len(y_train['Arrivals']) * 100}\")\n\npercent of imputed arrival data that is negative: 16.29023429779082\n\n\n\n\n\nWe created the following visualizations of our data and our best model’s (Decision Tree trained on `df_dropna_t9) predictions. The following plot shows how the number of refugee arrivals changes over time.\n\nimport matplotlib.pyplot as plt\ncounts = (df.groupby(['Year', 'Month'])['Arrivals'].sum().reset_index()).copy()\n# Set figure size\nplt.figure(figsize=(18, 6))\n\n# Combine year and month to create a new column for x-axis\ncounts['Year-Month'] = counts['Year'].astype(str) + '/' + counts['Month'].astype(str)\ncounts = counts.sort_values(by=['Year-Month', 'Arrivals'], ascending=[True, True])\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor i in range(len(counts['Year-Month'])):\n    update = counts['Year-Month'][i]\n    updateSlice = update[-1]\n    updateMonth =  months[int(updateSlice)]\n    counts['Year-Month'][i] = counts['Year-Month'][i][0:4] + '/' + updateMonth\n\n# Set x-axis ticks and labels\nx_ticks = counts['Year-Month'].unique()\nx_labels = counts['Month'].unique()\n\n# Loop through each year and plot the number of orders as bars\nfor year in counts['Year'].unique():\n    year_data = counts[counts['Year'] == year]\n    plt.bar(year_data['Year-Month'], year_data['Arrivals'], label=str(year), alpha=0.7)\n\n# Set x-axis ticks and labels\nplt.xticks(ticks=range(len(x_ticks)))\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=90, ha='right')\n\n# Add labels and title\nplt.xlabel('Month')\nplt.ylabel('Number of Arrivals')\nplt.title('Number of Arrivals by Month Over Time')\nplt.legend(title='Year')\n\n\n\n\n\n\n\n\nWe see that many of the arrivals occur in the summer months and that these arrival numbers are significantly larger than in the winter months. This trend seems to hold before and after the COVID-19 pandemic. We also see that there are more arrivals starting from 2017 onwards, this might be due to better data collection, but it is hard to know for sure.\nWe can also visualize how our model does in predicting arrivals. First, we build our best model and calculate the differences between its predictions and the true vales.\n\n# build our best model\ntrain, test = train_test_split(df_dropna_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\n\nDT = DecisionTreeRegressor()\nDT.fit(X_train, y_train)\n\npreds = DT.predict((X_test))\ny_true = y_test.values.flatten()\ndiffs = preds - y_true\n\nWhen we plot a histogram of the differences between our best models predictions and the true values, we can see that the model makes very few significantly wrong predictions (a prediction off by 10 or more people). The plot is on a logarithmic x-axis, to show the number of mispredictions within one order of magnitude. Only mispredictions of 10 people or more are plotted. This supports our hypothesis that the large RMSE is coming from only a few predictions that are very wrong.\n\n# plot only the differences greater than 100\n\nbins = [10, 100, 1000, 10000, 100000]\nplt.hist(diffs, bins=bins, edgecolor='black')\nplt.xscale('log')\n\n# Add labels and title\nplt.xlabel('Difference')\nplt.ylabel('Frequency')\nplt.title('Histogram of Differences')\n\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we can calculate the percent of of the time that the model made a perfect prediction, i.e. predicted the exact number of arrivals. We can see that 98% of the time, the model correctly predicted the exact number of arrivals, further supporting the hypothesis the model is very accurate and the large RMSE comes from a few bad predictions that were very off.\n\nprint(f'percent of perfect predictions: {(np.count_nonzero(diffs == 0) / len(diffs))}')\n\npercent of perfect predictions: 0.9805450474793485"
  },
  {
    "objectID": "posts/project-blog-post/index.html#abstract",
    "href": "posts/project-blog-post/index.html#abstract",
    "title": "UNCHR Machine Learning Challenge",
    "section": "",
    "text": "This project seeks to develop, implement, and evaluate various machine learning models to accurately predict refugee population movements in Somalia as part of the United Nations Commissioner for Refugees (UNHCR) Machine Learning Challenge. The data used by our models’ was collected by the UNHCR between 2015 and 2024 in Somalia. It contains information on population movements, markets, health, conflicts, and climate. We developed a web scraper to collect this data and combine it all into a single dataframe. To deal with missing data values, we created two datasets, one by dropping the missing values and the other by imputing them. Then, we built a Logistic Regression, Decision Tree, and Random Forest model on different subsets of these two datasets. We tested these models to see how accurately they could predict the number of refugee arrivals into a specific district in Somalia by computing the root mean squared error (RMSE), framing it as a regression problem. We found that the models had, on average, very high root mean squared errors (indicating a poor fit on the data), yet also had high R2 values (indicating a good fit on the data). We believe this discrepancy is explained by the fact that there are several very large outliers present in the data. To validate this, we binned the predictions into the three severity levels proposed by the UN, and treated it as a classification problem. When doing so, our best model, a Decision Tree Regressor trained on a subset of the dataset where missing features were dropped, achieved an accuracy of 99.5%. This model was also able to perfectly predict the exact number of arrivals 98% of the time, further supporting our hypothesis that these models fit the data very well and the high RMSE is explained by the existence of several large outliers."
  },
  {
    "objectID": "posts/project-blog-post/index.html#values-statement",
    "href": "posts/project-blog-post/index.html#values-statement",
    "title": "UNCHR Machine Learning Challenge",
    "section": "",
    "text": "The intended users of our project are the Somalian government and other NGOs that support refugees in the area. However, our project deals directly with people, as we are predicting where we expect refugees to move in Somalia. Because of this, the entire population of refugees in Somalia could potentially be affected by our project, if it were to be used by any of these agencies or governments to inform resource allocations. Both refugees in Somalia and the groups that support these refugees stand to benefit from our project, assuming the models are an effective tool. If this is the case, then these agencies can better allocate resources, which will benefit the refugees themselves as they will have access to more resources. Additionally, the government and agencies supporting them will benefit from more efficient resource movement. Conversely, if our models prove to be inaccurate enough that they cause resources to be misallocated, both refugees and the groups supporting them could be seriously harmed. Additionally, we note the possibility that bad actors could co-opt these models to predict refugee movements for malicious purposes.\nWe chose to work on this project because we were interested in the intersection between machine learning and public policy. When we found the UN challenge related to this project, we became very interested in the cause of this project, and found the ability to help refugees and their support groups compelling. Assuming that our models are an effective tool for helping agencies allocate resources for refugees, we believe that the benefits to refugee populations and their support groups will bring some relief to that region."
  },
  {
    "objectID": "posts/project-blog-post/index.html#introduction",
    "href": "posts/project-blog-post/index.html#introduction",
    "title": "UNCHR Machine Learning Challenge",
    "section": "",
    "text": "Our program addresses the humanitarian crisis in Somalia related to refugee movements and forcibly displaced people. The ability to accurately predict patterns in forcibly displaced peoples movements is crucial to the development of public policies that provide aid and resources to refugees. This is not a novel problem, and in fact has been approached several times before. Suleimenova et al., Gulden et al., and Lin et al. all developed agent based models to predict population movements. Gleick investigated the relationship between water and conflict in Syria, drawing a connection between water scarcity and conflicts, which in turn can drive population movements. Finally, Finnley noted an increase in migration of women and children during the severe drought of 1983-1985.\nWe focused on building models trained on a variety of data, including climate, conflicts, and market prices. By looking at a variety of conditions affecting human movement, we predict the movement patterns of displaced people in Somalia. The goal of this project is to effectively predict these population movements so that agencies supporting refugee populations in this area can better allocate resources to regions that are expected to receive a high number of migrants."
  },
  {
    "objectID": "posts/project-blog-post/index.html#materials-and-methods",
    "href": "posts/project-blog-post/index.html#materials-and-methods",
    "title": "UNCHR Machine Learning Challenge",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom models import dropNA\nfrom models import impute\nfrom models import evaluate_LR\nfrom models import evaluate_DT\nfrom models import evaluate_RF\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nAll of our data came from the Food Security and Nutrition Analysis Unit Dashboard for Somalia (FSNAU). This organization collects data related to food and security in Somalia. FSNAU provides access to this data through a public facing dashboard. Users can select what data they would like to look at and which time frames (in six month increments).\nFor our data, focused on creating two datasets, one where we drop missing values and one where we impute them. Within these datasets, we further split them into all available features, and just the top nine features as recommended by the UNHCR in their previous work with this data (project Jetson). Due to data availability, we focused on data from 2015 - 2024.\n\n# load data\ndf = pd.read_csv('data/combined_data.csv')\n\n\n\n\nThe goal of our models was to predict the number of refugees arriving in a certain district in Somalia. We used the “Arrivals” column of our data as the target feature for our models to predict. For training features, we either trained our models on all features available on the FSNAU dashboard, or the nine features recommended by the UNHCR (region, district, month, year, rainfall, number of conflict fatalities, number of conflict incidents, water price, and goat price). We chose to run models trained on these nine features because the UN found these particularly helpful when building their own machine learning models (Jetson Data). For all models, we dropped the feature describing the number of departures because such data can only be collected after the fact. That is, at the time this model would be run in a “real” scenario, there would be no way of knowing the number of departures in a district for the entire month; a month must first pass for this data to become known. Finally, we split the data into train and test groups, with 80% of the data in the train split and 20% of the data in the test split. Depending on the dataset used, the exact number of rows varied, but for the dataset we found to be best (dropping missing values and only using the nine best features), we had 51,812 train rows and 12,953 test rows.\n\n# create four dataframes of the data\ndf_dropna = dropNA(df)\ndf_dropna_t9 = dropNA(df, top_9=True)\ndf_impute = impute(df)\ndf_impute_t9 = impute(df, top_9=True)\n\nWe trained a Linear Regression model, Decision Tree Regressor, and Random Forest Regressor model from Sci-Kit Learn to make our predictions. These models were chosen because predicting the number of refugees arriving is a regression problem, where we want our model to give us an estimate of the number of arrivals in a district. We trained each model on the four datasets we created: drop missing values and use all features, drop missing values and use only the best nine features, impute missing values and use all features, and impute missing values and use only the nine best features. The reason for choosing to both impute and drop missing values is to determine the best approach to deal with these missing values, of which there were many. In fact, out of the 328,856 available rows, only 230 had no missing values. A single missing value in any one feature can render the entire row obsolete in training or testing. The benefit of imputing the data is that we end up with much more data to train and test with, potentially increasing accuracy and decreasing the possibility of overfitting. The drawback is that the data is generally lower quality, as many values are estimates not actual observations. Dropping missing values, on the other hand, leads to a smaller dataset, potentially decreasing accuracy and increasing the possibility of overfitting. The data we are left with, however, is much higher quality since it is composed entirely of actual observations.\nThe four datasets that result from this process, as well as their sizes are below:\n\ndf_dropna includes all rows without a single Na or NaN in any feature.\n\ntrain size: 230 x 23\ntest size: 55 x 23\n\ndf_dropna_t9 includes all rows without a single Na or NaN in any of the top nine features.\n\ntrain size: 51,812 x 10\ntest size: 12,953 x 10\n\ndf_impute includes all rows and features and fills in missing values by estimating their value using SciKitLearn’s SimpleImputer.\n\ntrain size: 263084 x 23\ntest size: 65,772 x 23\n\ndf_impute_t9 includes all rows and only the best nine features, and fills in missing values by estimating their value using SciKitLearn’s SimpleImputer.\n\ntrain size: 263084 x 10\ntest size: 65,772 x 10\n\n\nThe disadvantage of imputing the data can be seen in the fact that it produces negative values in the “Arrivals” column, which is impossible.\nWe then evaluated each of the models on each data set using R2, RMSE, and the classification accuracy for the three alarm levels. This classification accuracy came from binning the predictions into one of the three population movement alarm levels set by FSNAU and calculating the percent of times the model predicted the correct alarm level. The reason we included this metric is because the dataset contained several large outliers that heavily skewed the RMSE, which we will go into more at the end of this section. Additionally, we figured that in a real world scenario, it is more likely the aid groups would care more about whether the incoming number of migrants is low, medium, or high, rather than an exact number."
  },
  {
    "objectID": "posts/project-blog-post/index.html#results",
    "href": "posts/project-blog-post/index.html#results",
    "title": "UNCHR Machine Learning Challenge",
    "section": "",
    "text": "We evaluate the models using three different metrics, R2, RMSE, and the alarm level classification accuracy. The R2 value measures how well the model explains the observed data values. RMSE measures the average error for a prediction, with the interpretation being that the RMSE value is the average number of people the predictions are off by. Finally, the classification accuracy measures how often the models correctly predict the alarm level. In other words, the classification accuracy is how often the models can correctly predict whether the arrivals will be less than 1000 people, between 1000 and 5000 people, or greater than 5000 people. The tables below show the model performances when evaluated with each of these metrics on the testing data.\n\n\n\n# dropna\nprint(\"Drop Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_dropna, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_LR(X_train, y_train, X_test, y_test)\n\n# dropna top 9\nprint(\"Drop Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_dropna_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_LR(X_train, y_train, X_test, y_test)\n\n# impute\nprint(\"Impute Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_impute, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_LR(X_train, y_train, X_test, y_test)\n\n# impute top 9\nprint(\"Impute Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_impute_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_LR(X_train, y_train, X_test, y_test)\n\nDrop Missing Values, Use All Features:\nscore: -0.5120080054803344\nrmse: 4004.408898337328\nclassification accuracy: 1.0\n\n\nDrop Missing Values, Use Best Nine Features:\nscore: 0.022127246581247872\nrmse: 5531.715825977417\nclassification accuracy: 0.46383077279394735\n\n\nImpute Missing Values, Use All Features:\nscore: 0.33164398083006263\nrmse: 3641.15860937481\nclassification accuracy: 0.6846834519248313\n\n\nImpute Missing Values, Use Best Nine Features:\nscore: 0.09979970878891353\nrmse: 4044.098286302458\nclassification accuracy: 0.6812321352551237\n\n\n\n\n\n\n\n\n# dropna\nprint(\"Drop Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_dropna, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_DT(X_train, y_train, X_test, y_test)\n\n# dropna top 9\nprint(\"Drop Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_dropna_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_DT(X_train, y_train, X_test, y_test)\n\n# impute\nprint(\"Impute Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_impute, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_DT(X_train, y_train, X_test, y_test)\n\n# impute top 9\nprint(\"Impute Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_impute_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_DT(X_train, y_train, X_test, y_test)\n\nDrop Missing Values, Use All Features:\nscore: -1.0293524315764553\nrmse: 1832.0020891214767\nclassification accuracy: 0.9454545454545454\n\n\nDrop Missing Values, Use Best Nine Features:\nscore: 0.9086362235559209\nrmse: 1686.7363933175686\nclassification accuracy: 0.9932834092488226\n\n\nImpute Missing Values, Use All Features:\nscore: 0.9141093025870605\nrmse: 1243.3944680840762\nclassification accuracy: 0.8610654989965335\n\n\nImpute Missing Values, Use Best Nine Features:\nscore: 0.8600721201782545\nrmse: 1622.8755875767067\nclassification accuracy: 0.8631940643434897\n\n\n\n\n\n\n\n\n# dropna\nprint(\"Drop Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_dropna, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_RF(X_train, y_train, X_test, y_test)\n\n# dropna top 9\nprint(\"Drop Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_dropna_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_RF(X_train, y_train, X_test, y_test)\n\n# impute\nprint(\"Impute Missing Values, Use All Features:\")\ntrain, test = train_test_split(df_impute, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_RF(X_train, y_train, X_test, y_test)\n\n# impute top 9\nprint(\"Impute Missing Values, Use Best Nine Features:\")\ntrain, test = train_test_split(df_impute_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\nevaluate_RF(X_train, y_train, X_test, y_test)\n\nDrop Missing Values, Use All Features:\nscore: -0.41771466376163113\nrmse: 1925.9235858153875\nclassification accuracy: 0.9272727272727272\n\n\nDrop Missing Values, Use Best Nine Features:\nscore: 0.963600818038985\nrmse: 1194.1962938556157\nclassification accuracy: 0.9922797807457732\n\n\nImpute Missing Values, Use All Features:\nscore: 0.9530521565094427\nrmse: 971.2710461948245\nclassification accuracy: 0.8653834458432159\n\n\nImpute Missing Values, Use Best Nine Features:\nscore: 0.9251482120105943\nrmse: 1137.0446510214172\nclassification accuracy: 0.8624186583956699\n\n\n\n\n\n\n\nThe dataset contained several large outliers for our target column (“Arrivals”) that were throwing off the RMSE calculations. The 95th percentile of arrivals was 3,701; in other words 90% of arrivales were 3,701 or fewer people. However, the maximum number of arrivals for a single observation was 259,678. This implies the existence of some large outliers in the dataset that skew the RMSE calculation, making the models seem to perform worse than they actually do.\n\nprint(f'90th percentile: \\n{df[\"Arrivals\"].quantile(q=0.95)} \\n')\nprint(f'arrivals statistics: \\n{df[\"Arrivals\"].describe()}')\n\n90th percentile: \n3701.0 \n\narrivals statistics: \ncount    191416.000000\nmean       1226.451807\nstd        5534.244627\nmin           1.000000\n25%          49.000000\n50%         194.000000\n75%         614.000000\nmax      259678.000000\nName: Arrivals, dtype: float64\n\n\n\n\n\nWhen we impute the data, we are filling in the missing values with an estimate based on the other present features. However, when we impute this dataset, we actually end up with some negative values in the arrival column, which is impossible. This can be problematic because it can teach the models to predict negative arrivals, which are impossible observations. We can see that 16% of our imputed data is negative.\n\ndef count_negative(values):\n    count = 0\n    for value in values:\n        if value &lt; 0:\n            count += 1\n    return count\n\nprint(f\"percent of imputed arrival data that is negative: {count_negative(df_impute['Arrivals'])/ len(y_train['Arrivals']) * 100}\")\n\npercent of imputed arrival data that is negative: 16.29023429779082\n\n\n\n\n\nWe created the following visualizations of our data and our best model’s (Decision Tree trained on `df_dropna_t9) predictions. The following plot shows how the number of refugee arrivals changes over time.\n\nimport matplotlib.pyplot as plt\ncounts = (df.groupby(['Year', 'Month'])['Arrivals'].sum().reset_index()).copy()\n# Set figure size\nplt.figure(figsize=(18, 6))\n\n# Combine year and month to create a new column for x-axis\ncounts['Year-Month'] = counts['Year'].astype(str) + '/' + counts['Month'].astype(str)\ncounts = counts.sort_values(by=['Year-Month', 'Arrivals'], ascending=[True, True])\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor i in range(len(counts['Year-Month'])):\n    update = counts['Year-Month'][i]\n    updateSlice = update[-1]\n    updateMonth =  months[int(updateSlice)]\n    counts['Year-Month'][i] = counts['Year-Month'][i][0:4] + '/' + updateMonth\n\n# Set x-axis ticks and labels\nx_ticks = counts['Year-Month'].unique()\nx_labels = counts['Month'].unique()\n\n# Loop through each year and plot the number of orders as bars\nfor year in counts['Year'].unique():\n    year_data = counts[counts['Year'] == year]\n    plt.bar(year_data['Year-Month'], year_data['Arrivals'], label=str(year), alpha=0.7)\n\n# Set x-axis ticks and labels\nplt.xticks(ticks=range(len(x_ticks)))\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=90, ha='right')\n\n# Add labels and title\nplt.xlabel('Month')\nplt.ylabel('Number of Arrivals')\nplt.title('Number of Arrivals by Month Over Time')\nplt.legend(title='Year')\n\n\n\n\n\n\n\n\nWe see that many of the arrivals occur in the summer months and that these arrival numbers are significantly larger than in the winter months. This trend seems to hold before and after the COVID-19 pandemic. We also see that there are more arrivals starting from 2017 onwards, this might be due to better data collection, but it is hard to know for sure.\nWe can also visualize how our model does in predicting arrivals. First, we build our best model and calculate the differences between its predictions and the true vales.\n\n# build our best model\ntrain, test = train_test_split(df_dropna_t9, test_size=0.2)\nX_train = train.drop(['Arrivals'], axis=1)\ny_train = train[['Arrivals']]\nX_test = test.drop(['Arrivals'], axis=1)\ny_test = test[['Arrivals']]\n\nDT = DecisionTreeRegressor()\nDT.fit(X_train, y_train)\n\npreds = DT.predict((X_test))\ny_true = y_test.values.flatten()\ndiffs = preds - y_true\n\nWhen we plot a histogram of the differences between our best models predictions and the true values, we can see that the model makes very few significantly wrong predictions (a prediction off by 10 or more people). The plot is on a logarithmic x-axis, to show the number of mispredictions within one order of magnitude. Only mispredictions of 10 people or more are plotted. This supports our hypothesis that the large RMSE is coming from only a few predictions that are very wrong.\n\n# plot only the differences greater than 100\n\nbins = [10, 100, 1000, 10000, 100000]\nplt.hist(diffs, bins=bins, edgecolor='black')\nplt.xscale('log')\n\n# Add labels and title\nplt.xlabel('Difference')\nplt.ylabel('Frequency')\nplt.title('Histogram of Differences')\n\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we can calculate the percent of of the time that the model made a perfect prediction, i.e. predicted the exact number of arrivals. We can see that 98% of the time, the model correctly predicted the exact number of arrivals, further supporting the hypothesis the model is very accurate and the large RMSE comes from a few bad predictions that were very off.\n\nprint(f'percent of perfect predictions: {(np.count_nonzero(diffs == 0) / len(diffs))}')\n\npercent of perfect predictions: 0.9805450474793485"
  },
  {
    "objectID": "posts/project-blog-post/index.html#group-contributions",
    "href": "posts/project-blog-post/index.html#group-contributions",
    "title": "UNCHR Machine Learning Challenge",
    "section": "Group Contributions",
    "text": "Group Contributions\n\nMihir\nI worked on some of the data collection, finding the specific datasets that we would use as well as working on finding ways to scrape and combine the CSVs. However, I primarily spent my time working on the models. This meant finding ways to process our data so that it was usable on logistic regression as well as working on encoding and imputing the data. Jamie and I built the functions to build, train, and evaluate our models in a file called models.py. Here, we had to address multiple issues including figuring out why our score values were so low and deciding what methods of evaluating for accuracy we would use. Do overcome these challenges, we looked out the outliers in our data and found a plausible explanation and created our own evaluation function. I also wrote the code to visualize the number of arrivals and the code to understand the disadvantages of imputing the data. Finally, all three of us worked on the presentation and I mainly wrote the abstract and approach section while working with Jamie on the results section.\n\n\nJamie\nI helped build the web scraper and data aggregator functions in the source code. I helped write some of the code that built and evaluated the models in the blog post as well as visualized the models predictions. I helped write the Values Statement and Introduction, as well as parts of the results and conclusion.\n\n\nJake\nI helped build a web scraper for market data. When we started building the models, I helped work on the ideal way to clean up the data, including imputing and removing NAs. I helped build and expand a few of the early models and expand on Jamie’s original neural network. I helped with the presentation and for our blog, I wrote our results section and our conclusion, drawing upon the data we found."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "from matplotlib import pyplot as plt\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nAbstract\nThe code for the model can be found here\nIn this blog post, I implemented the popular classification algorithm Logistic Regression with both spicy and vanilla gradient descent. I conducted three experiments. In the first, I found that with no momentum (beta value) vanilla gradient descent was shown to find the optimal weight value for the model as the loss converged at its smallest value. The next experiment expanded on gradient descent by adding momentum and a faster learning rate, which kept the same accuracy, but improved the efficiency of the model’s convergence—this was an implementation of “spicy” gradient descent. I then explored the effects of overfitting. I created training and testing datasets with more dimensions (features) than data points, and the resulting accuracy showed a 100% accuracy for training data but a lower accuracy (94%) for test data. Within these experiments, I was able to see the complexities of logistic regression, its advantages of using gradient descent to find an optimal weight and understand/visualize how overfitting can hinder a model.\n\n\nExperiments\nIn order to test out logistic regression and spicy gradient decent, we need code to generate data for our classification.\nWe will generate points, wherein the number of dimensions controls the number of features and noise represents the difficulty of the problem.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.3)\n\nWe can also plot this data:\n\ndef plot(X, y, ax):\n    targets = [0, 1]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], edgecolors = \"grey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5)\n    ax.set(xlabel = \"x1\", ylabel = \"x2\")\n\nfigure, ax = plt.subplots(1, 1)\n\n\nplot(X, y, ax)\n\n\n\n\n\n\n\n\nNow that we have the points and the algorithm implemented in logistic.py, we need to train our model:\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(10000):\n    # add other stuff to e.g. keep track of the loss over time. \n    opt.step(X, y, alpha = 0.1, beta = 0.9)\n\n\n\nVanilla Gradient Descent\nUsing our training loop, we will test and plot out the decision boundary and use a modified training loop, keeping track of the loss to achieve a correct weight vector.\nWe will keep track of the loss each time so we can graph it and visualize its progression overtime.\nWe will use 2 dimensions, a small alpha value and beta = 0:\n\nlosses = []\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(10000):\n    # add other stuff to e.g. keep track of the loss over time. \n    losses.append(opt.step(X, y, alpha = 0.1, beta = 0.0))\n\nNow we can compute the accuracy by comparing our predictions to the target vector and plot the decision boundary:\n\n#plot decision region with a draw line function - courtesy of Prof Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfigure, ax = plt.subplots(1, 1)\nplot(X, y, ax)\nweight =  LR.w\ndraw_line(weight, -0.5, 1.5, ax)\n\n\n\n\n\n\n\n\n\nprint(\"Accuracy: \", torch.mean(1.0 * (LR.predict(X) == y)).item())\n\nAccuracy:  0.9900000095367432\n\n\nOur accuracy comes out to 0.99 with 10,000 steps in our loop. The decision boundary, given a moderate to low amount of noise seems to create a correct decision boundary between the two classifications.\nSince we kept track of the loss, we can plot it’s progression overtime:\n\n# plotting loss and final loss\nplt.figure(figsize=(12, 4))\nplt.plot(losses)\nplt.title('Vanilla Gradient Descent Loss')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\n\nprint(\"Loss at the final step: \", losses[-1].item())\n\n\n\n\n\n\n\n\nLoss at the final step:  0.04300102964043617\n\n\nOur loss seems to be correctly decreasing with each step, slowly converging to 0. This shows our weight vector is creating the appropriate decision boundary.\n\n\nExperimenting with momentum\nUsing the same data, by using momentum (changing our beta value to 0.9 for example) can help us converge our weight vector with fewer steps than if beta = 0 (vanilla gradient descent).\nFirstly, I want to experiment with an optimal alpha value given a constant beta, then I want to compare the best alpha value with our spicy gradient descent with the vanilla gradient descent in the last experiment. The following code is the same as the last training loop, but has an outer loop that keeps track of the final loss values after 10000 iterations for alpha values ranging from 0.01 to 0.25 in 0.01 increments.\n\nlossesAlpha = []\nfinal_losses_momentum = []\n\nLR_Momentum = LogisticRegression() \nopt_momentum = GradientDescentOptimizer(LR_Momentum)\n\nfor j in range(1, 27):\n    lossesMomentum = []\n    for _ in range(10000):\n        # add other stuff to e.g. keep track of the loss over time. \n        lossesAlpha.append(opt_momentum.step(X, y, alpha = float(j) / 100, beta = 0.9))\n    final_losses_momentum.append(lossesAlpha[-1])\n\nNow we can visualize the final loss for each alpha value and see which one is optimal with beta = 0.9\n\n# plotting loss and final loss\nplt.figure(figsize=(12, 4))\nplt.plot(final_losses_momentum, marker=\"o\")\nplt.xticks([x for x in range(0, len(final_losses_momentum))])\nplt.title('Final loss after 500 iterations for different alpha values')\nplt.xlabel('alpha value (*100)')\nplt.ylabel('Final Loss')\nplt.show()\n\nprint(\"Final loss for alpha = 0.25:\", final_losses_momentum[-1].item())\n\n\n\n\n\n\n\n\nFinal loss for alpha = 0.25: 0.0396832711994648\n\n\nIt appears that after 10000 iterations, our final loss value approaches its lowest for an alpha value of 0.25, without much change after that. In this case, with a beta value of 0.9, we can increase the alpha value to 0.25, which is slightly more efficient than keeping it at 0.1 (or 10 on the graph).\nNow we will execute our training loop with 10000 iterations like before, plot the decision region, and find the accuracy\n\nlossesMomentum = []\n\nLR_Momentum = LogisticRegression() \nopt_momentum = GradientDescentOptimizer(LR_Momentum)\n\nfor _ in range(10000):\n    # add other stuff to e.g. keep track of the loss over time. \n    lossesMomentum.append(opt_momentum.step(X, y, alpha = 0.25, beta = 0.9))\n\n\nfigure, ax = plt.subplots(1, 1)\nplot(X, y, ax)\nweight =  LR_Momentum.w\ndraw_line(weight, -0.5, 1.5, ax)\n\nprint(\"Accuracy with momentum: \", torch.mean(1.0 * (LR_Momentum.predict(X) == y)).item())\n\n\nAccuracy with momentum:  0.9900000095367432\n\n\n\n\n\n\n\n\n\nWe retain the same accuracy of 99% after the same amount of iterations with our new alpha and beta values. Now we can plot the change in loss to compare the models with vanilla descent and spicy descent:\n\nplt.figure(figsize=(10, 5))\nplt.plot(lossesMomentum, label = \"Gradient descent with momentum\")\nplt.plot(losses, label=\"Vanilla gradient descent\")\nplt.legend()\nplt.title('Vanilla Gradient Descent Loss vs Spicy Gradient Descent Loss over time')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.show()\n\n\n\n\n\n\n\n\nOur plot shows that the spicy gradient descent (descent with momentum or a beta value of 0.9) converges at a faster rate than a model with vanilla gradient descent, therefore this model is faster and more efficient.\n\n\nOverfitting\nIn our last experiment, we will explore overfitting,a nd for that we need data where the number of dimensions (features) is greater than the number of points. This will show how overfitting our model on the testing data will drastically decrease the accuracy even when the training data has high accuracy.\nIn this case, I will generate a test and train data with 20 points and 100 dimensions. We will also increase the noise to make the problem harder.\n\nX_train, y_train = classification_data(n_points = 20, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 20, noise = 0.5, p_dims = 100)\n\nNow we can use this data to train a new model. This is the same as before, but we want to keep track of the accuracy of both the test and train model over the course of the iterations.\n\nlossesOF = []\n\nLR_OF = LogisticRegression() \nopt_OF = GradientDescentOptimizer(LR_OF)\n\n#\naccuracy_test = []\naccuracy_train = []\n\nfor _ in range(10000):\n    # add other stuff to e.g. keep track of the loss over time. \n    lossesOF.append(opt_OF.step(X_train, y_train, alpha = 0.25, beta = 0.9))\n\n    accuracy_test.append(torch.mean(1.0 * (LR_OF.predict(X_test) == y_test)))\n    accuracy_train.append(torch.mean(1.0 * (LR_OF.predict(X_train) == y_train)))\n\n\nprint(\"Final training accuracy:\", torch.mean(1.0 * (LR_OF.predict(X_train) == y_train)).item())\nprint(\"Final testing accuracy:\", torch.mean(1.0 * (LR_OF.predict(X_test) == y_test)).item())\n\n\nFinal training accuracy: 1.0\nFinal testing accuracy: 0.949999988079071\n\n\nThe final training accuracy comes out to 100%, but because of overfitting, we are given the testing accuracy decreased to 94%.\nWe can see how much this accuracy changes over our iterations:\n\nplt.plot(accuracy_test, label=\"Test data\")\nplt.plot(accuracy_train, label=\"Training data\")\nplt.legend()\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Iterations\")\n\nplt.title(\"Accuracies of overfit training and testing data over 10000 iterations with Spicy Gradient Descent\",fontsize = 10)\nplt.show()\n\n\n\n\n\n\n\n\nWith this graph, we can see how the training data is almost immediately 100% accurate, and our testing data flattens out at 94% accuracy, which shows how overfitting with too many features and not enough data for classification can skew the data and accuracy.\n\n\nDiscussion\nUsing gradient descent is a good way of calculating the descent of loss for the classification algorithm Logistic Regression. It’s process is to recalculate the weight of the model over the iterations in the training loop. While vanilla gradient is a good algorithm for finding the optimal weight, We were able to see the benefits of “momentum” as well as increasing the learning rate to make the model more efficient. Logistic Regression with Spicy Gradient Descent is a great method for classification, but by increasing the amount of dimensions and decreasing the amount of data to work with, we were able to see how this leads to overfitting with a perfect training accuracy but a subsequently lower testing accuracy.\nDuring this blog post, I learned about the implementation of a classification algorithm and how it builds upon basic Linear Models. I was also able to greater understand the benefits of using gradient descent in minimizing loss and finding the optimal weights, as well as the dangers of possibly overfitting data. While accuracy and loss is a good way to examine the accuracy of a model, it might not reveal the whole picture."
  },
  {
    "objectID": "posts/replication-study/index.html",
    "href": "posts/replication-study/index.html",
    "title": "Replication Study - Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "Abstract\nThis post attempts to replicate the study Dissecting racial bias in an algorithm used to manage the health of populations by Obermeyer et al (2019). We looked at figures 1 and 3 in the paper that maps out racial disparities in the health care system. We will also looked at the disparities between black and white patients with 5 or fewer chronic illnesses. We are able to find the algorithm perpetuates racial disparities, as black patients are given less risk scores even with the same number of chronic illnesses. This decreases the likelihood of black patients getting appropriate care compared to the same conditions in white patients. With the complexity of the healthcare system, which has its own sets of disparities, this algorithm only reveals the systems that disproproitantely affect black people.\n\n\nPart A\nWe will use a randomized version of the data provided by the study so we can analyze the same types of patterns:\n\nimport pandas as pd\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n\n\nPart B: Reproducing Figure 1\nWe will reproduce Figure 1A of Fig. 1A of Obermeyer et al. (2019) in order to visuzlize risk score penalties against mean number of chronic conditions in that percentile. We will attempt to show the disparities between races in calculating risk scores for how many illenesses they may have.\n\n# Create a new percentile column based on the risk score\ndf['risk_percentile'] = pd.qcut(df['risk_score_t'], q=100, labels=False, duplicates='drop')\n\n#getting a mean number of illnesses\nmean_illness_percentiles = df.groupby(['risk_percentile', 'race'])['gagne_sum_t'].mean().reset_index()\n\npercentile_plot = sns.scatterplot(data=mean_illness_percentiles, x='gagne_sum_t', y='risk_percentile', hue='race', style='race')\npercentile_plot.set_title('Algorithm Risk Percentile vs Mean Number of Chronic Illnesses')\npercentile_plot.set_xlabel('Mean Number of Illnesses')\npercentile_plot.set_ylabel('Risk Score Percentile')\n\nText(0, 0.5, 'Risk Score Percentile')\n\n\n\n\n\n\n\n\n\nThe first figure shows there is a clear disparity between black and white patients’ scores. When we compare their actual mean number of illnesses against the scores, Black patients generally have a lower risk score for the same given number of illnesses. This means Black patients need to be more sick (more illnesses) to be referred to a care program.\n\n\nPart C: Reproducing Figure 3\nFigure 3 in the sutdy looks at the mean medical expenditure to percentile of the risk score. It also calculated the mean total expentiture to the number of chronic illnesses.\n\n# again making risk percentile\ndf['risk_percentile'] = pd.qcut(df['risk_score_t'], q=100, labels=False, duplicates='drop')\n\n\ncost_risk_percentiles = df.groupby(['risk_percentile', 'race'])['cost_t'].mean().reset_index()\ncost_illness_percentiles = df.groupby(['gagne_sum_t', 'race'])['cost_t'].mean().reset_index()\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\nsns.scatterplot(data=cost_risk_percentiles, x='risk_percentile', y='cost_t', hue='race', style='race', ax=axs[0])\nsns.scatterplot(data=cost_illness_percentiles, x='gagne_sum_t', y='cost_t', hue='race', style='race', ax=axs[1])\n\n\naxs[1].set_ylabel('')\naxs[1].set_xlabel('# Chronic Illnesses')\naxs[1].set_yscale('log')\naxs[0].set_ylabel('Total Medical Expenditure')\naxs[0].set_xlabel('Percentile Risk Score')\naxs[0].set_yscale('log')\n\nfig.suptitle(\"Costs versus algorithm-predicted risk, and costs versus health for race\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn the left figure for figure 3, we can see that black patients average lower medical expenditure per risk score and the outliers are mostly black patients. The second figure shows as the number of chronic illnesses goes up, there is a general trend towards disparity. Black patients tend to get fewer costs than white patients at lower number of illnesses, but eventually this trend falls apart, possibly due to less data available at higher illness counts.\n\n\nPart D: Modeling Cost Disparity\nGiven the disparity we just observed, we can explore patients that have 5 or fewer chronic illnesses.\nWe need to prep the data by determining the percentage of patients with 5 or fewer chronic illnesses:\n\npercentage_chronic = len(df.loc[df['gagne_sum_t'] &lt;= 5])/ len(df) * 100\npercentage_chronic\n\n95.53952115447689\n\n\nWith 95% of patients that have 5 or fewer active chronic illnesses, we will have a good idea of common racial disparity trends.\nNow we want to make a new column to see the logarithm of the cost, but only look at costs above 0:\n\ndf_model = df.copy()\ndf_model = df_model[df_model['cost_t'] &gt; 0]\ndf_model['log_cost'] = np.log(df_model['cost_t'])\ndf_model['log_cost'] = np.log(df_model['cost_t'])\n\nWe should also encode the race categories into numeric values. 0 correlates to white and 1 correlates to black.\n\ndf_model['race_binary'] = (df_model['race'] == 'black') * 1\n\nWe will also separate data into the predictions X and the targets y. The predictor variables are based on race and # of chronic illnesses.\n\ndf_X = df_model[['race_binary','gagne_sum_t']]\ndf_y = df_model[['log_cost']]\n\n\n\nModeling\nWe can implement a linear regression model, but because this relationship is nonlinear we need to add polynomial features. To find out how many features we need to add, we can use a loop provided by Prof Chodrow:\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nscores = []\n\nfor i in range(20):\n  X_ = add_polynomial_features(df_X, i)\n  LR = LinearRegression()\n  LR.fit(X_, df_y)\n  cv_score = cross_val_score(LR, X_, df_y, cv=5)\n  scores.append((i, cv_score.mean()))\n\n\nprint(max(scores, key=lambda x: x[1]))\n\n(10, 0.14830062340213399)\n\n\nIt seems that degree 10 is the highest so we will use that to make a new feature matrix and fit our model:\n\nX_poly = add_polynomial_features(df_X, 10)\nLR = LinearRegression()\nLR.fit(X_poly, df_y)\n\nprint(X_poly)\nprint(LR.coef_)\n\n       race_binary  gagne_sum_t  poly_1  poly_2  poly_3  poly_4  poly_5  \\\n0                0            0       0       0       0       0       0   \n1                0            3       3       9      27      81     243   \n2                0            0       0       0       0       0       0   \n3                0            0       0       0       0       0       0   \n4                0            1       1       1       1       1       1   \n...            ...          ...     ...     ...     ...     ...     ...   \n48779            0            0       0       0       0       0       0   \n48780            0            1       1       1       1       1       1   \n48781            0            0       0       0       0       0       0   \n48782            0            3       3       9      27      81     243   \n48783            0            0       0       0       0       0       0   \n\n       poly_6  poly_7  poly_8  poly_9  \n0           0       0       0       0  \n1         729    2187    6561   19683  \n2           0       0       0       0  \n3           0       0       0       0  \n4           1       1       1       1  \n...       ...     ...     ...     ...  \n48779       0       0       0       0  \n48780       1       1       1       1  \n48781       0       0       0       0  \n48782     729    2187    6561   19683  \n48783       0       0       0       0  \n\n[46887 rows x 11 columns]\n[[-2.67114875e-01  5.08816323e-01  5.08816458e-01 -1.03056778e+00\n   5.88024794e-01 -1.77621999e-01  3.11556830e-02 -3.27057852e-03\n   2.01927004e-04 -6.74580399e-06  9.39007116e-08]]\n\n\nPrinting out our new coef_ of the linear regression, we see that the race coefficient was the first column and correlates with -2.67. Now we can calculate the wb value:\n\nwb = np.exp(LR.coef_[0][0])\nwb\n\n0.7655851197936364\n\n\nOverall, we find that the cost incurred by black patients is 76.5% compared to white patients with 5 or less chronic illnesses. This supports the argument from the study that black patients have lower costs because they do not receive as much needed healthcare, revealing racial biases.\n\n\nDiscussion\nThrough replicating these figures, I found a racial bias. Black patients with the same illnesses in real life are given lower risk scores, making their care less of a priority and unable to enroll in care programs. This disparity may be due to existing racial disparities that prevent healthcare access—therefore cheaper healthcare—to black patients.\nIn terms of the sufficiency outlined by Barocas, Hardt, and Narayanan (2023), the algorithm here gives differing outcomes dependent on race, even though they should be the same. Black patients are less qualified for healthcare even with less chronic conditions.\nThe algorithm may also fail the test of independence. Since health is tightly bound by healthcare costs and other systems that perpetuate racial disparities, the score is not as independent as it should be."
  },
  {
    "objectID": "posts/optimal-decision-making/index.html",
    "href": "posts/optimal-decision-making/index.html",
    "title": "Optimal Decision-making",
    "section": "",
    "text": "Abstract\nIn this blog post, I implemented a classification system based on loans that finds an optimal profit for a bank. But through this exploration, I also visualize how these systems can affect the borrower negatively and how it touches on the issues of fairness. How do we balance efficient and “optimal” systems that benefit the institutions in power, but also understand how they can perpetuate oppression and unfairness? By using Logistic Regression and finding the best features to train on, I was able to find the ideal threshold for the probability of repayment, which determines which borrowers have access to credit. Once finding the optimal weight and threshold, I see the implications of this threshold both on the banks and the borrowers, and how from differing perspectives, the same outcomes can have completely different meanings of fairness. I was able to find that the optimal threshold score is 0.53, and that the bank’s optimal profit is $32,368,726.\n\n\nGrabbing and exploring the data\nFirst I need to download the training data:\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n7.29\n0\n0.02\nN\n17\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n5.42\n0\n0.09\nN\n4\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n11.71\n0\n0.25\nN\n4\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n12.68\n0\n0.24\nN\n8\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\nN\n4\n\n\n\n\n26064 rows × 12 columns\n\n\n\n\nWe can start exploring the data. Let’s look at how the home ownership type is broken down with the different types of loans for housing:\n\nsummaryTable = df_train.groupby(['person_home_ownership']).aggregate(Loan_Average =('loan_percent_income', 'mean'))\nsummaryTable\n\n\n\n\n\n\n\n\n\nLoan_Average\n\n\nperson_home_ownership\n\n\n\n\n\nMORTGAGE\n0.151560\n\n\nOTHER\n0.194432\n\n\nOWN\n0.188800\n\n\nRENT\n0.181819\n\n\n\n\n\n\n\n\nIt seems like the loan percentage is lowest for mortgages. Those who already have a mortgage or own houses are more likely to need less as a percentage of their total wealth for loans and have more security in their wealth.\nWe can also visualize this data:\n\n\n# plot\nsns.boxplot(df_train, x='person_home_ownership', y='loan_percent_income').set(xlabel =\"House Ownership type\", ylabel = \"Income percentile\", title ='House ownership vs Income percentile')\n\n[Text(0.5, 0, 'House Ownership type'),\n Text(0, 0.5, 'Income percentile'),\n Text(0.5, 1.0, 'House ownership vs Income percentile')]\n\n\n\n\n\n\n\n\n\nInterestingly, the boxplot shows us that generally those who already own property will take loans as more of a percentage of their income. Interestingly, the mortgages has less percentage of income to go towards loans. If someone is already willing to spend money on a mortgage, they have more income to spare.\nWe can also look at age and see whether or not the bank determines if the loan is likely to be repaid depending on this factor. First we can copy our data and create quartiles of ages .\n\ndf_age_c = df_train.copy()\n\nquartiles = df_age_c['person_age'].quantile([0.25, 0.5, 0.75])\nq1 = f\"{df_age_c['person_age'].min()} to {quartiles[0.25]}\"\nq2 = f\"{quartiles[0.25]} to {quartiles[0.50]}\"\nq3 = f\"{quartiles[0.50]} to {quartiles[0.75]}\"\nq4 = f\"{quartiles[0.75]} to {df_age_c['person_age'].max()}\"\ndf_age_c['age_quartile'] = pd.qcut(df_train['person_age'], q = [0, 0.25, 0.5, 0.75, 1], labels = [q1, q2, q3, q4])\n\n# now we can plot\nsns.boxplot(df_age_c, x='age_quartile', y='loan_amnt').set(xlabel =\"Age quartile\", ylabel = \"Loan amount ($)\", title ='Age quartile vs Amount Received in Loans')\n\n/Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\n[Text(0.5, 0, 'Age quartile'),\n Text(0, 0.5, 'Loan amount ($)'),\n Text(0.5, 1.0, 'Age quartile vs Amount Received in Loans')]\n\n\n\n\n\n\n\n\n\nWhile there is not much change in the ages of how much income you spend on loans, maybe a better metric is to see the time spend employed to how much one is willing to spend loans as a percentage of their income. We will do the same quartile method:\n\ndf_train_emp = df_train.copy()\n\nquartiles = df_train_emp['person_emp_length'].quantile([0.25, 0.5, 0.75])\nq1 = f\"{df_train_emp['person_emp_length'].min()} to {quartiles[0.25]}\"\nq2 = f\"{quartiles[0.25]} to {quartiles[0.50]}\"\nq3 = f\"{quartiles[0.50]} to {quartiles[0.75]}\"\nq4 = f\"{quartiles[0.75]} to {df_train_emp['person_emp_length'].max()}\"\ndf_train_emp['employment_length_quartile'] = pd.qcut(df_train_emp['person_emp_length'], q = [0, 0.25, 0.5, 0.75, 1], labels = [q1, q2, q3, q4])\n\nsns.boxplot(df_train_emp, x='employment_length_quartile', y='loan_amnt').set(xlabel =\"Employment length quartile\", ylabel = \"Amount received in loans\", title ='Employment quartile vs Amount received in loans')\n\n/Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\n[Text(0.5, 0, 'Employment length quartile'),\n Text(0, 0.5, 'Amount received in loans'),\n Text(0.5, 1.0, 'Employment quartile vs Amount received in loans')]\n\n\n\n\n\n\n\n\n\nFrom this boxplot, we can see that people that have been working for longer are more willing to take more money out in loans. The change is gradual, but there is a large difference between the two ranges, meaning the data deals with a wide range of circumstances.\n\n\nBuilding the Model\nWe will implement Logistic Regression and use cross validation to determine the 5 best features and prevent overfitting.\nWe also want to use a labelEncoder to label non-numeric data as numeric values so it can be fit with Logistic Regression. I also need to drop NA values\n\ndf_LR = df_train.copy()\ndf_LR.drop(columns = 'loan_grade', inplace = True)\n\n# Also get rid of outlier in age feature\ndf_LR = df_LR[df_LR['person_age'] != 144]\n\n# getting all columns (features)\ndf_LR = df_LR[list(df_LR.columns)]\n\n\n# Need to encode features that are not numeric\n# home_ownership, loan_intent, default_on_file\n\nencoder = LabelEncoder()\n\nencoder.fit(df_LR['person_home_ownership'])\ndf_LR['person_home_ownership'] = encoder.transform(df_LR['person_home_ownership'])\n\nencoder.fit(df_LR['loan_intent'])\ndf_LR['loan_intent'] = encoder.transform(df_LR['loan_intent'])\n\nencoder.fit(df_LR['cb_person_default_on_file'])\ndf_LR['cb_person_default_on_file'] = encoder.transform(df_LR['cb_person_default_on_file'])\n\ndf_LR = df_LR.dropna()\ndf_LR['person_emp_length'] = df_LR['person_emp_length'].astype(int)\n\ndf_LR.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\n3\n3\n1\n11750\n13.47\n0\n0.12\n1\n6\n\n\n2\n22\n36996\n3\n5\n1\n10000\n7.51\n0\n0.27\n0\n4\n\n\n3\n24\n26000\n3\n2\n3\n1325\n12.87\n1\n0.05\n0\n4\n\n\n4\n29\n53004\n0\n2\n2\n15000\n9.63\n0\n0.28\n0\n10\n\n\n6\n21\n21700\n3\n2\n2\n5500\n14.91\n1\n0.25\n0\n2\n\n\n\n\n\n\n\n\nNow with our cleaned up data, I want to find the combination of the 5 best features by calculating the score:\n\n# train model based on five columns and choose best one\nbestScore = 0\nBestFeatures = []\n\n#Get all columns and remove target\ncolumns = list(df_LR.columns)\ncolumns.remove('loan_status')\n\nfor feat in combinations(columns, 5):\n    \n    features = list(feat)\n\n    LR = LogisticRegression(max_iter=2000)\n\n    LR.fit(df_LR[features], df_LR['loan_status'])\n\n    score = LR.score(df_LR[features], df_LR['loan_status'])\n\n    if score &gt; bestScore:\n      bestScore = score\n      BestFeatures = features\n\nNow we can see which combination of 5 features gives us the best score for Linear Regression:\n\nprint(\"The best score: \", bestScore)\nprint(\"Best features: \", BestFeatures)\n\nThe best score:  0.8462277331470486\nBest features:  ['person_age', 'person_home_ownership', 'person_emp_length', 'loan_percent_income', 'cb_person_cred_hist_length']\n\n\nWe can also do cross validation to see if there is any overfitting:\n\ncross_val_scores = cross_val_score(LR, df_LR[features], df_LR['loan_status'], cv=5)\n#print((cv_scores_LR).mean())\ncross_val_scores.mean()\n\n0.795973448332728\n\n\nOur cross validation mean gives us 80% accuracy with the best features being age, income, home ownership, loan intent and loan status.\nWe need to train our model then get the weight vector:\n\n# train\nLR.fit(df_LR[BestFeatures], df_LR['loan_status'])\n\n#print weight vector for all features\nprint(LR.coef_[0])\n\n[-8.07133566e-04  3.36542470e-01 -1.99919415e-02  7.93852091e+00\n -1.88709181e-03]\n\n\n\n\nFinding a threshold\nWe have trained a Logistic Regression model and have a weight vector, and now we need to find a suitable threshold which determines our target value — whether someone is likely to default on a loan.\nWe have a couple assumptions to determine a bank’s profit:\nIf the loan is repaid in full, the profit for the bank is equal to loan_amnt(1 + 0.25loan_int_rate)10 - loan_amnt OR If the borrower defaults on the loan, the “profit” for the bank is equal to loan_amnt(1 + 0.25loan_int_rate)3 - 1.7*loan_amnt\nSince we need to find a feature that will pass this threshold, a good option is to calculate whether the probability of an individual pays their loans back passes the threshold. We can use the predict_proba function with our model and add it to our dataframe:\nA way to find an optimal threshold is to graph an expected gain.\n\nproba_pay = LR.predict_proba(df_LR[BestFeatures]) # method courtesy of Mihir Singh\ndf_LR['proba_pay'] = proba_pay[:,0].tolist()\n\nNow we can use expected gain to find the optimal threshold by graphing it based on the bank profit, using the two aforementioned assumptions/equations:\n\ndef getProfit(t, df):\n\n    #new dataframe for any row that is above threshold\n    df_thresh = df_LR[df_LR['proba_pay'] &gt; t].copy()\n\n    profit = 0\n    #default = 0\n\n    for i in range(len(df_thresh)):\n\n        # if NOT DEFAULTED\n        if df_thresh.iloc[i]['loan_status'] == 0:\n            profit += df_thresh.iloc[i]['loan_amnt'] * (1 + 0.25 *  (df_thresh.iloc[i]['loan_int_rate'] / 100))**10 - (df_thresh.iloc[i]['loan_amnt'])\n        else:\n            profit += df_thresh.iloc[i]['loan_amnt'] * (1 + 0.25 * (df_thresh.iloc[i]['loan_int_rate'] /100)) **3 - (1.7 * df_thresh.iloc[i]['loan_amnt']) \n    return profit\n\n\nthresholds = np.linspace(0, 1, 100)\nprofits = []\nfor t in thresholds:\n    profits.append(getProfit(t, df_LR))\n\n\nsns.set_theme(rc={'figure.figsize':(11.7,8.27)})\nplt.title(\"Threshold vs Maximum Profit\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit\")\nplt.plot(thresholds, profits)\nplt.figure(figsize=(10, 6))\nplt.show()\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nOur threshold for maximum profit converges at a highest point in between 0.4 and 0.5, and we can extract the exact value, the total profit at that threshold and the profit per borrower:\n\nprint(\"Max threshold: \", profits.index(max(profits)) / 100)\nprint(f\"Max profit at ideal threshold: ${int(max(profits))}\")\nprint(f\"Profit per borrower: ${max(profits) / len(df_LR[df_LR['proba_pay'] &gt;= 0.53])}\")\n\nMax threshold:  0.53\nMax profit at ideal threshold: $32406222\nProfit per borrower: $1566.9562640408653\n\n\n\n\nEvaluating from bank’s perspective\nNow that we have an optimal weight and threshold, we can explore the profit on some test data:\n\n# import data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test_original = pd.read_csv(url)\n\n\n#clean up data\ndf_test = df_test_original.copy()\ndf_test.drop(columns = 'loan_grade', inplace = True)\ndf_test = df_test[df_test['person_age'] != 144]\n\n\n# Like before, use Label Encoder so Logistic Regression can evaluate with numeric labels\nencoder = LabelEncoder()\n\nencoder.fit(df_test['cb_person_default_on_file'])\ndf_test['cb_person_default_on_file'] = encoder.transform(df_test['cb_person_default_on_file'])\n\nencoder.fit(df_test['loan_intent'])\ndf_test['loan_intent'] = encoder.transform(df_test['loan_intent'])\n\nencoder.fit(df_test['person_home_ownership'])\ndf_test['person_home_ownership'] = encoder.transform(df_test['person_home_ownership'])\n\n\n\ndf_test = df_test.dropna()\ndf_test['person_emp_length'] = df_test['person_emp_length'].astype(int)\n\n\nLike before, we will create a new column to determine the probability that someone will pay based on the best features chosen when we compared the scores\n\nBestFeatures\n\n['person_age',\n 'person_home_ownership',\n 'person_emp_length',\n 'loan_percent_income',\n 'cb_person_cred_hist_length']\n\n\n\nproba_pay = LR.predict_proba(df_test[BestFeatures])\ndf_test['proba_pay'] = proba_pay[:,0].tolist()\n\n#FIND PROFITS with our threshold of 0.53 and new dataframe\ntest_profit = getProfit(0.53, df_test)\n\nprint(f\"with a threshold of 0.53, the total bank profit: ${test_profit}\")\nprint(f\"profit per borrower: ${test_profit / len(df_test[df_test['proba_pay'] &gt;= 0.53])}\")\n\ndf_test.head()\n\n\n\nwith a threshold of 0.53, the total bank profit: $32368726.934103493\nprofit per borrower: $6309.693359474365\n\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\nproba_pay\n\n\n\n\n0\n21\n42000\n3\n5\n5\n1000\n15.58\n1\n0.02\n0\n4\n0.907719\n\n\n1\n32\n51000\n0\n2\n0\n15000\n11.36\n0\n0.29\n0\n9\n0.752253\n\n\n2\n35\n54084\n3\n2\n0\n3000\n12.61\n0\n0.06\n0\n6\n0.872545\n\n\n3\n28\n66300\n0\n11\n3\n12000\n14.11\n1\n0.15\n0\n6\n0.916299\n\n\n4\n22\n70550\n3\n0\n3\n7000\n15.88\n1\n0.08\n0\n3\n0.846674\n\n\n\n\n\n\n\n\n\n\nEvaluating the model from the borrower’s perspective\nGiven the bank’s perspective, its profit and profit per borrower, we can also use the data to explore the effects on the borrower.\n\nFirstly, we can examine how difficult it is for certain age groups to get access to credit. In order to do this, we can create a plot of the probability that each age group will pay.\n\nLike before, we also move ages into groups using quartiles:\n\n# group individuals into quartiles by age\nquartiles = df_test['person_age'].quantile([0.25, 0.5, 0.75])\n\nq1 = f\"{df_test['person_age'].min()} to {quartiles[0.25]}\"\nq2 = f\"{quartiles[0.25]} to {quartiles[0.50]}\"\nq3 = f\"{quartiles[0.50]} to {quartiles[0.75]}\"\nq4 = f\"{quartiles[0.75]} to {df_test['person_age'].max()}\"\n\n# separate individuals into quartiles\ndf_test['age_groups'] = pd.qcut(df_test['person_age'], q = [0, 0.25, 0.5, 0.75, 1], labels = [q1, q2, q3, q4])\n\n# plot\nsns.boxplot(x=df_test['age_groups'], y=df_test['proba_pay'])\nplt.title('Which age groups have the most access to credit')\nplt.ylabel('Probability of paying')\nplt.xlabel('Age group quartiles')\n\n/Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\nText(0.5, 0, 'Age group quartiles')\n\n\n\n\n\n\n\n\n\nBased on our plot, it seems that younger people are more likely to be denied credit and therefore have less access. After the youngest group, there is little difference between ages 23 and older.\n\nNext we will look at the difficulty of getting loans for medical expenses compared to other reasons, including business or education.\n\nFor this, we will decode the loan intent and group by each loan intent label to see loan status and the rate of acceptance given the Logistic Regression probability\n\ndf_medical = df_test.copy()\ndf_medical['loan_intent'] = df_test_original['loan_intent']\n\n# make new column for all those who are accepted for loans based on if they pass the threshold\ndf_medical['get_loans'] = df_medical['proba_pay'] &gt; 0.53\nget_loans = df_medical.groupby('loan_intent')['get_loans'].mean()\n\n# another column based on actual loan status (did they default or repay)\ndf_medical['repay_loans'] = df_medical['loan_status'] == 0\nrepay = df_medical.groupby('loan_intent')['repay_loans'].mean()\n\n\npd.DataFrame({'loans approved': get_loans, 'loans repaid': repay})\n\n\n\n\n\n\n\n\n\nloans approved\nloans repaid\n\n\nloan_intent\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.884956\n0.712389\n\n\nEDUCATION\n0.897109\n0.832483\n\n\nHOMEIMPROVEMENT\n0.933442\n0.750000\n\n\nMEDICAL\n0.876048\n0.715750\n\n\nPERSONAL\n0.890782\n0.779559\n\n\nVENTURE\n0.903527\n0.853734\n\n\n\n\n\n\n\n\nOut of all of the intents, medical loans have the least likelihood to get approved, whereas home improvement is the highest. While the difference is not drastic, medical loans are noticeably more likely to default—along with dept consolidation—compared to other loans. This may be due to the fact that medical loans and debt consolidation are done out of necessity and often lead to default more often, so they are harder to access.\n\nThe last experiment is to see how a person’s income level can affect how much they have access to credit given out model.\n\nSimilar to ages, we will use quartiles to group into 4 income categories then make a box plot:\n\nquartiles = df_test['person_income'].quantile([0.25, 0.5, 0.75])\n\nq1 = f\"{df_test['person_income'].min()} to {quartiles[0.25]}\"\nq2 = f\"{quartiles[0.25]} to {quartiles[0.50]}\"\nq3 = f\"{quartiles[0.50]} to {quartiles[0.75]}\"\nq4 = f\"{quartiles[0.75]} to {df_test['person_income'].max()}\"\ndf_test['income_groups'] = pd.qcut(df_test['person_income'], q = [0, 0.25, 0.5, 0.75, 1], labels = [q1, q2, q3, q4])\n\n# plot\nsns.boxplot(x=df_test['income_groups'], y=df_test['proba_pay'])\nplt.title('Income groups and Access to Credit')\nplt.ylabel('Access to credit')\nplt.xlabel(\"Income group quartiles\")\n\n/Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\nText(0.5, 0, 'Income group quartiles')\n\n\n\n\n\n\n\n\n\nWe can see there is a gradual but noticeable trend of access to credit for higher income groups. Those with money already available to spend can afford to take the risk of paying out later.\n\n\nDiscussion\nIn the end, by training our model, we were able to determine on the test set that the optimal threshold score is 0.53, and that the bank’s optimal profit is $32,368,726, given the risk of default. One fundamental question to address given the exploration is how fair this system of loans is or if it perpetuates existing systems of class oppression and income inequality. Looking at our 2nd exploration of the borrow’s perspective, medical loans are much harder to access. On the surface, this makes sense because they are usually at risk of default more than other loans. However, in the following exploration, we were clearly able to see how access to credit benefited the wealthy and groups with higher income than lower income groups. This means that not all groups are affected equally in this disparity for medical loans. Certain systems that seem “fair” and based off of a blanket meritocracy sometimes underneath are only perpetuating systems that benefit the already powerful. For medical loans, it is never the borrowers “fault” that they got hurt, but are often punished in the process, yet from the bank’s perspective, the only loss is profit. At a narrow view of fairness, this system makes sense, where these systems are merely a side effect of given trends. A wider view of fairness would tell us it would benefit us to try to see the benefits of changing our loan systems from reasons that are born out of fairness, not profit incentive.\nIn the end, the loans that are the riskiest are often the ones that are the most needed and vital for the wellbeing of people. In this case, I don’t think this system is fair—it punishes those who are already at a disadvantage."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\nAbstract\nBy using machine learning models, we can more easily classify the species of Palmer penguins given certain physical features—both quantitative and qualitative. Based on the exploration of data visualization, training and cross validation, it was concluded that a logistic regression model that was tested through cross validation using Culmen Depth (mm), Culmen Length (mm) and Island of origin (Biscoe, Dream or Torgersen) provided a 100% accurate classifier for future testing data.\nTo start the process, we need to implement the training data and our packages for data visualization.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nExploring Visualizations\nIn order to accurately predict the species of penguin, we need to determine the best subset of data and features to train on. To do this, we will look at some data visualization.\nFirst, we need to prepare the data. Given the data, we will drop some columns that we will not be able to train on and focus on either quantitative data (like culmen length/depth) or one-hot encoded columns, like sex or island of origin.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\n\nTo choose the best possible features for our model, it’s helpful to visualize the dat.\nFirstly, I want to find the one hot encoded feature which doesn’t contain any numeric values. Below I’m using a box plot to visualize the the difference in the culmen depths between islands.\n\nsns.boxplot(data=train, x=\"Island\", y=\"Culmen Depth (mm)\")\n\n\n\n\n\n\n\n\nFIGURE 1: In the above figure, we can see that there is enough variance between the islands to ensure this feature could be viable. This is especially apparent on Biscoe island, which may be due to a very specific population of penguin on that island.\nWe can also see how this changes between the different species.\n\nsns.boxplot(data=train, x=\"Island\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n\n\n\n\n\n\n\nFIGURE 2: Here we can see the reason for the large difference in Biscoe island—when separating boxes by species. Biscoe island is the only one with the Gentoo penguin, which has a significantly smaller culmen depth. This visualization also tells us that island can be a good one-hot encoded feature and an indicator of the type of penguin.\nWe can also look at some other numerical features by comparing them to culmen depth.\n\nimport matplotlib.pyplot as plt\n\nax = plt.subplots(figsize=(12, 3))\nplot = sns.boxplot(train, x = \"Body Mass (g)\", y = \"Culmen Depth (mm)\", width=0.8)\nplot.tick_params(axis='x', rotation=90)\n\n\n\n\n\n\n\n\n\nFIGURE 3: As body mass increases, culmen depth increases for a certain range, but there seems to be a threshold where it drops, perhaps due to a unique body proportion of a species.\n\nplot = sns.boxplot(data=train, x=\"Species\", y=\"Body Mass (g)\")\nplot.tick_params(labelsize=6)\n\n\n\n\n\n\n\n\nFIGURE 4: The Gentoo penguin has a larger mass, but in figure 2, it has a notably smaller culmen depth. Therefore, we see no discernable trend between body mass and culmen depth. With how variant Culmen depth is, it may be a good numeric feature.\nHowever, we also need to see if culmen depth itself could be used as a good numeric feature. Additionally, we need a second numerical feature to ensure the quality of the training. To do this we can remove the features with low variance.\nWe can select an 80% variance threshold and plug our cleaned up data frame\n\nfrom sklearn.feature_selection import VarianceThreshold\nvariance = VarianceThreshold(threshold=(.8 * (1 - .8)))\n\nvariance.fit_transform(X_train)\n\n\n\nChoosing Our Model\nWe can loop through all combinations of columns of possible features we want to use. As of now, I may want to use Island as my one-hot-encoded feature and Culmen Depth as a numerical feature, but we will iterate through all feature combinations using itertools.\nI will test logistic regression, random forest classifier and a decision tree. For each model, I will keep track of the best score for the given feature combination.\nHowever, before I determine the accuracy of the decision tree, I need to find the best max_depth parameter. To do this, I will use cross validation and an exhaustive search between two possible depth values. We will test depths 5-20 for this example with a 7 fold cross validation.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\ndepths = []\nmax_score = 0\nbest_depth = 0\nfor i in range(5,20):\n    classifier = DecisionTreeClassifier(max_depth=i)\n    scores = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=7, n_jobs=4)\n    if scores.mean() &gt; max_score: \n        max_score = scores.mean()\n        best_depth = i\n    depths.append((i,scores.mean()))\n\n\nprint(depths)\nprint(max_score)\nprint(best_depth)\n\n[(5, 0.960960960960961), (6, 0.9649292149292149), (7, 0.9649292149292149), (8, 0.961068211068211), (9, 0.9649292149292149), (10, 0.965036465036465), (11, 0.961068211068211), (12, 0.9649292149292149), (13, 0.965036465036465), (14, 0.9688974688974689), (15, 0.9649292149292149), (16, 0.960960960960961), (17, 0.961068211068211), (18, 0.961068211068211), (19, 0.9688974688974689)]\n0.9688974688974689\n14\n\n\nFrom the search, it seems that a max depth of 7 gives the best average score when cross validating. Now we can search through each feature combination and train our three models:\n\nfrom itertools import combinations\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#keep track of the best score for each\nlogreg_score = 0\nlogreg_features = []\n\ndecision_score = 0\ndecision_features = []\n\nforest_score = 0\nforest_features = []\n\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Island\", \"Clutch Completion\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    #logistic regression\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    temp_score = LR.score(X_train[cols], y_train)\n    if temp_score &gt; logreg_score:\n      logreg_score = temp_score\n      logreg_features = cols\n\n    #RANDOM FOREST\n    RF = RandomForestClassifier()\n    RF.fit(X_train[cols], y_train)\n    temp_score = RF.score(X_train[cols], y_train)\n    if temp_score &gt; forest_score:\n      forest_score = temp_score\n      forest_features = cols\n\n    #DECISION TREE\n    DT = DecisionTreeClassifier(max_depth=7)\n    DT.fit(X_train[cols], y_train)\n    temp_score = DT.score(X_train[cols], y_train)\n    if temp_score &gt; decision_score:\n      decision_score = temp_score\n      decision_features = cols\n    \n\nprint(\"Logical regression score:\", logreg_score)\nprint(\"Logical regression best feautes\", logreg_features)\nprint()\nprint(\"Random forest score: \", forest_score)\nprint(\"Random forest best features\", forest_features)\nprint()\nprint(\"Decision tree score: \", decision_score)\nprint(\"Decision tree best features\", decision_features)\n\nLogical regression score: 0.99609375\nLogical regression best feautes ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\nRandom forest score:  1.0\nRandom forest best features ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\nDecision tree score:  1.0\nDecision tree best features ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nFrom the initial classifications, the random forest and decision trees seem to give 100% testing accuracy and all three models agree that island, culmen length and culmen depth are the best options for features. However, we need to do some cross validation to prevent over-fitting.\n\ncv_lr = cross_val_score(LR, X_train, y_train, cv=7).mean()\ncv_rf = cross_val_score(RF, X_train, y_train, cv=7).mean()\ncv_dt = cross_val_score(DT, X_train, y_train, cv=7).mean()\n\nprint(\"logistic regression: \", cv_lr)\nprint(\"Random Forest: \", cv_rf)\nprint(\"Decision Tree: \", cv_dt)\n\nlogistic regression:  1.0\nRandom Forest:  0.9882024882024882\nDecision Tree:  0.9649292149292149\n\n\nThe result of our cross validation reveals that logistic regression has a 100% accuracy. We now know we can create a reliable logistic regression model with our three features: Culmen Depth, Culmen Length and Island.\n\n\nTesting\nWe will now test our model with a new dataset, which produces a perfect prediction for the testing data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ncolumns = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[columns], y_train)\nLR.score(X_test[columns], y_test)\n\n1.0\n\n\n\n\nPlotting Decision Regions\nNow that we have our accurate model and test data, we want to plot our decision regions.\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[columns], y_train)\n\n\n\n\n\n\n\n\nPlotting our decision regions provides a good visualization of our model and the reasoning behind its decisions. For each island, there is enough distinction between our quantitative features to provide accurate classification.\n\n\nConfusion Matrix\nAnother way to get a good indication of our model and its accuracy is to create a confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\nprediction = LR.predict(X_test[columns])\n\nconfusion_matrix = confusion_matrix(y_test, prediction)\n\nprint(confusion_matrix)\n\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\n\nThe result of our confusion matrix shows only values on the diagonal axis, revealing the models accuracy.\n\n\nDiscussion\nBased on my exploration of various models and features, I concluded that a logistic regression model trained with high variance features Culmen Depth, Culmen Length and one qualitative feature, Island, produces a 100% accurate model for classifying Palmer penguins. One thing that I learned from this exploration is the importance of steps like cross validation. Although on the surface, some models— for example our decision tree—we’re more accurate, cross validation reveals they were less so due to over-fitting and the nature of the dataset. I also learned the importance of data visualization in finding correct features and discerning trends in the data. Also revealing the decision regions of the model helps me understand why certain features work. There are many steps for determining the right classification model, and many of these steps can lead to interesting and surprising discoveries."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Welcome to my Machine Learning blog. Below you will find implementations of various ML algorithms and data explorations, including a submission for the UNCHR Machine Learning Challenge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUNCHR Machine Learning Challenge\n\n\n\n\n\nUNCHR Machine Learning Challenge - Predicting the movement of displaced peoples in Somalia\n\n\n\n\n\nMay 20, 2024\n\n\nJake Gilbert, Mihir Singh, and Jamie Hackney\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study - Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations\n\n\n\n\n\nReplicating a study on the racial disparities in a health risk algorithm\n\n\n\n\n\nMay 15, 2024\n\n\nJake Gilbert\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nImplementing the perceptron algorithm, testing multiple dimensions and batching\n\n\n\n\n\nMay 12, 2024\n\n\nJake Gilbert\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Decision-making\n\n\n\n\n\nImplement a classifcation system that maximises bank rofit and finds the optimal threshold for credit access\n\n\n\n\n\nMay 10, 2024\n\n\nJake Gilbert\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nExploring the Logistic Regression algorithm, vanilla and spicy gradient descent and the effects of overfitting\n\n\n\n\n\nMay 5, 2024\n\n\nJake gilbert\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Music Genre Classification\n\n\n\n\n\nUsing deep learning and neural networks to classify music genres\n\n\n\n\n\nMay 2, 2024\n\n\nJake gilbert\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins based on quantitative and qualitative data\n\n\n\n\n\nFeb 22, 2024\n\n\nJake Gilbert\n\n\n\n\n\n\nNo matching items"
  }
]